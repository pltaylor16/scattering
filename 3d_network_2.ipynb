{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b758fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 58945, 1, 1)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import pi as pi\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import tensorflow\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "\n",
    "policy = mixed_precision.Policy('float32')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "size = 16\n",
    "\n",
    "\n",
    "# Probably need to rewrite this but use v0 as the random variable\n",
    "def get_rand_tetra_loc(num_tetra, v0, v1, v2, v3, Flip = False):\n",
    "   \n",
    "    trans = np.zeros((3, num_tetra))\n",
    "    trans[0,:] = np.random.uniform(-5.,5., num_tetra)\n",
    "    trans[1,:] = np.random.uniform(-5.,5., num_tetra)\n",
    "    trans[2,:] = np.random.uniform(-5.,5., num_tetra)\n",
    "\n",
    "    rot_arr = R.random(num = num_tetra).as_matrix()\n",
    "   \n",
    "    if Flip == True:\n",
    "        #flip_idx = np.random.choice(num_tetra, int(num_tetra/2), replace=False)\n",
    "        v0_flip, v1_flip, v2_flip, v3_flip = -v0.copy(), -v1.copy(), -v2.copy(), -v3.copy()\n",
    "       \n",
    "    tetra = np.zeros((4,3,num_tetra))\n",
    "    for i in range(num_tetra):\n",
    "        if Flip == True:\n",
    "            tetra[0,:,i] = np.dot(rot_arr[i,:,:], v0_flip) + trans[:,i]\n",
    "            tetra[1,:,i] = np.dot(rot_arr[i,:,:], v1_flip) + trans[:,i]\n",
    "            tetra[2,:,i] = np.dot(rot_arr[i,:,:], v2_flip) + trans[:,i]\n",
    "            tetra[3,:,i] = np.dot(rot_arr[i,:,:], v3_flip) + trans[:,i]\n",
    "\n",
    "        else:\n",
    "            tetra[0,:,i] = np.dot(rot_arr[i,:,:], v0) + trans[:,i]\n",
    "            tetra[1,:,i] = np.dot(rot_arr[i,:,:], v1) + trans[:,i]\n",
    "            tetra[2,:,i] = np.dot(rot_arr[i,:,:], v2) + trans[:,i]\n",
    "            tetra[3,:,i] = np.dot(rot_arr[i,:,:], v3) + trans[:,i]\n",
    "               \n",
    "       \n",
    "    return tetra\n",
    "\n",
    "\n",
    "\n",
    "num_left_tet = 3\n",
    "num_right_tet = 1\n",
    "v0 = np.array([8,0,0])\n",
    "v1 = np.array([0,4,0])\n",
    "v2 = np.array([0,0,2])\n",
    "v3 = np.array([0,0,0])\n",
    "\n",
    "\n",
    "def get_rand_tet_excess(num_left_tet, num_right_tet, v0, v1, v2, v3, master_flip = False):\n",
    "    n_tet = num_left_tet + num_right_tet\n",
    "    dat = np.zeros((4,3,n_tet))\n",
    "    if master_flip == False:\n",
    "        dat[:,:,:num_left_tet] = get_rand_tetra_loc(num_left_tet, v0, v1, v2, v3, Flip = False)\n",
    "        dat[:,:,num_left_tet:] = get_rand_tetra_loc(num_right_tet, v0, v1, v2, v3, Flip = True)\n",
    "    else:\n",
    "        dat[:,:,:num_left_tet] = get_rand_tetra_loc(num_left_tet, v0, v1, v2, v3, Flip = True)\n",
    "        dat[:,:,num_left_tet:] = get_rand_tetra_loc(num_right_tet, v0, v1, v2, v3, Flip = False)      \n",
    "    return dat\n",
    "\n",
    "\n",
    "\n",
    "def get_ran_field(tetra):\n",
    "   \n",
    "    num_tetra = np.shape(tetra)[2]\n",
    "    verts = np.zeros((4 * num_tetra, 3))\n",
    "    for v in range(4):\n",
    "        for n in range(num_tetra):\n",
    "            verts[4*n+v,:] = tetra[v,:,n]\n",
    "\n",
    "    npix = size\n",
    "    mn = -size\n",
    "    mx = size\n",
    "    pix_sz = (mx - mn) / npix\n",
    "\n",
    "    grid = np.zeros((size,size,size))      \n",
    "    for v in range(4 * num_tetra):\n",
    "        x = verts[v,0]\n",
    "        y = verts[v,1]\n",
    "        z = verts[v,2]\n",
    "        xpix = int((x - mn) / pix_sz )\n",
    "        ypix = int((y - mn) / pix_sz )\n",
    "        zpix = int((z - mn) / pix_sz )\n",
    "        grid[xpix, ypix, zpix] += 1\n",
    "    return grid\n",
    "\n",
    "\n",
    "test = get_ran_field(get_rand_tet_excess(num_left_tet, num_right_tet, v0, v1, v2, v3)) \n",
    "\n",
    "\n",
    "from scatter import Scatter3D\n",
    "\n",
    "J = 4\n",
    "L = 4\n",
    "M = 16\n",
    "slant1 = 4./L\n",
    "slant2 = 2./L\n",
    "\n",
    "ntrain = 1000\n",
    "nvalid = 100\n",
    "\n",
    "nbatch_train = 20\n",
    "nbatch_valid = 10\n",
    "batch_size_train = int(ntrain / nbatch_train)\n",
    "batch_size_valid = int(nvalid / nbatch_valid)\n",
    "\n",
    "\n",
    "xtrain, xvalid = np.zeros((batch_size_train,size,size,size)), np.zeros((batch_size_valid,size,size,size))\n",
    "\n",
    "\n",
    "inp_train = (size, size, size)\n",
    "inp_valid = inp_train\n",
    "\n",
    "Scat = Scatter3D(J,L,M,slant1, slant2)\n",
    "filter_bank = Scat.filter_bank()\n",
    "\n",
    "\n",
    "\n",
    "x = tensorflow.ones((10,16,16,16))\n",
    "\n",
    "\n",
    "phi = filter_bank['phi']\n",
    "psi = filter_bank['psi']\n",
    "max_order = 3\n",
    "coeffs = Scat.compute_coefs_no_pad_extend(x, phi, psi, max_order, out_type = 'array')\n",
    "print (np.shape(coeffs))\n",
    "\n",
    "compressed_shape = np.shape(coeffs)[2]\n",
    "\n",
    "\n",
    "Xtrain, Xtrain_flip = np.zeros((ntrain,compressed_shape)), np.zeros((ntrain,compressed_shape)) \n",
    "for i in range(nbatch_train):\n",
    "    print (i)\n",
    "    for j in range(batch_size_train):\n",
    "        xtrain[j,:,:,:] = get_ran_field(get_rand_tet_excess(num_left_tet, num_right_tet, v0, v1, v2, v3))\n",
    "    xtrain_flip = xtrain[:,::-1,:,:]\n",
    "    Xtrain[i*batch_size_train:(i+1)*batch_size_train] =\\\n",
    "    Scat.compute_coefs_no_pad_extend(xtrain, phi, psi, max_order, out_type = 'array')[:,0,:,0,0]\n",
    "    Xtrain_flip[i*batch_size_train:(i+1)*batch_size_train] =\\\n",
    "    Scat.compute_coefs_no_pad_extend(xtrain_flip, phi, psi, max_order, out_type = 'array')[:,0,:,0,0]\n",
    "    \n",
    "    \n",
    "Xvalid, Xvalid_flip = np.zeros((nvalid,compressed_shape)), np.zeros((nvalid,compressed_shape)) \n",
    "for i in range(nbatch_valid):\n",
    "    print (i)\n",
    "    for j in range(batch_size_valid):\n",
    "        xvalid[j,:,:,:] = get_ran_field(get_rand_tet_excess(num_left_tet, num_right_tet, v0, v1, v2, v3))\n",
    "    xvalid_flip = xvalid[:,::-1,:,:]\n",
    "    Xvalid[i*batch_size_valid:(i+1)*batch_size_valid] =\\\n",
    "    Scat.compute_coefs_no_pad_extend(xvalid, phi, psi, max_order, out_type = 'array')[:,0,:,0,0]\n",
    "    Xvalid_flip[i*batch_size_valid:(i+1)*batch_size_valid] =\\\n",
    "    Scat.compute_coefs_no_pad_extend(xvalid_flip, phi, psi, max_order, out_type = 'array')[:,0,:,0,0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9566ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.shape(Xtrain)[1]\n",
    "from tensorflow.keras.layers import Dense, Input, Subtract\n",
    "from keras.models import Model\n",
    "\n",
    "A1 = Input(shape=(s1), name='input layer 1')\n",
    "B1 = Input(shape=(s1), name='input layer 2')\n",
    "\n",
    "\n",
    "d1 = Dense(100, activation = 'relu')\n",
    "A2 = d1(A1)\n",
    "B2 = d1(B1)\n",
    "\n",
    "\n",
    "d2 = Dense(100, activation = 'relu')\n",
    "A4 = d2(A2)\n",
    "B4 = d2(B2)\n",
    "\n",
    "\n",
    "\n",
    "output = Dense(1, activation = 'linear', name = 'Output')\n",
    "A6 =  output(A4)\n",
    "B6 =  output(B4)  \n",
    "\n",
    "\n",
    "out = Subtract()([A6,B6])\n",
    "\n",
    "merged3 = Model(inputs=[A1, B1],outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4acc648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(tensorflow.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def call(self, y_true, y_pred):        \n",
    "        return -tensorflow.math.reduce_mean(y_pred) / tensorflow.math.reduce_std(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afeaf542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 39ms/step - loss: -4.1951e-04 - val_loss: 0.0903\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 1s 35ms/step - loss: -0.0395 - val_loss: 0.1112\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 1s 35ms/step - loss: -0.0557 - val_loss: 0.1407\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 1s 37ms/step - loss: -0.0610 - val_loss: 0.1492\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 1s 38ms/step - loss: -0.0492 - val_loss: 0.1625\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 1s 37ms/step - loss: -0.0745 - val_loss: 0.1684\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 1s 40ms/step - loss: -0.0744 - val_loss: 0.1818\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 1s 39ms/step - loss: -0.0837 - val_loss: 0.1718\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 1s 40ms/step - loss: -0.0786 - val_loss: 0.1604\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 1s 41ms/step - loss: -0.0728 - val_loss: 0.1571\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 1s 43ms/step - loss: -0.0826 - val_loss: 0.1802\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 1s 45ms/step - loss: -0.0843 - val_loss: 0.1844\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 1s 45ms/step - loss: -0.0866 - val_loss: 0.1760\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 1s 46ms/step - loss: -0.0894 - val_loss: 0.1754\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 1s 47ms/step - loss: -0.0955 - val_loss: 0.1759\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 1s 48ms/step - loss: -0.0928 - val_loss: 0.1743\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 1s 49ms/step - loss: -0.0826 - val_loss: 0.1762\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 1s 49ms/step - loss: -0.0917 - val_loss: 0.1737\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 1s 49ms/step - loss: -0.0841 - val_loss: 0.1677\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 1s 49ms/step - loss: -0.0903 - val_loss: 0.1816\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 1s 50ms/step - loss: -0.0873 - val_loss: 0.1629\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0928 - val_loss: 0.1775\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 1s 50ms/step - loss: -0.0865 - val_loss: 0.1765\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0680 - val_loss: 0.1553\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: -0.0858 - val_loss: 0.1778\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0948 - val_loss: 0.1857\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0821 - val_loss: 0.1829\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: -0.0839 - val_loss: 0.1617\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: -0.0816 - val_loss: 0.1796\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0924 - val_loss: 0.1847\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0945 - val_loss: 0.1878\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0885 - val_loss: 0.1864\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0892 - val_loss: 0.1822\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0816 - val_loss: 0.1641\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: -0.0875 - val_loss: 0.1858\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0968 - val_loss: 0.1846\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0980 - val_loss: 0.1819\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0820 - val_loss: 0.1815\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0748 - val_loss: 0.1533\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: -0.0836 - val_loss: 0.1658\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0932 - val_loss: 0.1831\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0992 - val_loss: 0.1882\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0924 - val_loss: 0.1836\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0931 - val_loss: 0.1853\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0955 - val_loss: 0.1683\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0987 - val_loss: 0.1735\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.1003 - val_loss: 0.1783\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0905 - val_loss: 0.1824\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0979 - val_loss: 0.1914\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.1016 - val_loss: 0.1870\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0726 - val_loss: 0.1819\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0886 - val_loss: 0.1752\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0986 - val_loss: 0.1807\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0943 - val_loss: 0.1857\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: -0.1038 - val_loss: 0.1744\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.1005 - val_loss: 0.1884\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.1102 - val_loss: 0.1828\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0891 - val_loss: 0.1817\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0985 - val_loss: 0.1780\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.1023 - val_loss: 0.1825\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.1010 - val_loss: 0.1814\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: -0.1130 - val_loss: 0.1740\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0910 - val_loss: 0.1818\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 1s 50ms/step - loss: -0.0954 - val_loss: 0.1745\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0992 - val_loss: 0.1867\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.0989 - val_loss: 0.1897\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.1058 - val_loss: 0.1773\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0991 - val_loss: 0.1832\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 1s 50ms/step - loss: -0.0664 - val_loss: 0.1468\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.1039 - val_loss: 0.1855\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: -0.1044 - val_loss: 0.1903\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: -0.0993 - val_loss: 0.1840\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 1s 61ms/step - loss: -0.1033 - val_loss: 0.1949\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 1s 65ms/step - loss: -0.1024 - val_loss: 0.1893\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: -0.1055 - val_loss: 0.1785\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 1s 65ms/step - loss: -0.1051 - val_loss: 0.1933\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: -0.1044 - val_loss: 0.1880\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 1s 57ms/step - loss: -0.1024 - val_loss: 0.1842\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: -0.1045 - val_loss: 0.1860\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 1s 63ms/step - loss: -0.1055 - val_loss: 0.1881\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: -0.1026 - val_loss: 0.1888\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: -0.1015 - val_loss: 0.1707\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: -0.0958 - val_loss: 0.1849\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 1s 65ms/step - loss: -0.1065 - val_loss: 0.1723\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 1s 57ms/step - loss: -0.1046 - val_loss: 0.1843\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 1s 61ms/step - loss: -0.0913 - val_loss: 0.1901\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 1s 57ms/step - loss: -0.1034 - val_loss: 0.1811\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 1s 65ms/step - loss: -0.1075 - val_loss: 0.1909\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: -0.1113 - val_loss: 0.1888\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: -0.1027 - val_loss: 0.1743\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 1s 55ms/step - loss: -0.1029 - val_loss: 0.1763\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 1s 58ms/step - loss: -0.1150 - val_loss: 0.1863\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: -0.1066 - val_loss: 0.1756\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: -0.0976 - val_loss: 0.1826\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: -0.0944 - val_loss: 0.1618\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 1s 56ms/step - loss: -0.0986 - val_loss: 0.1838\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 1s 68ms/step - loss: -0.1119 - val_loss: 0.1914\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: -0.1133 - val_loss: 0.1803\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 1s 69ms/step - loss: -0.0996 - val_loss: 0.1968\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 1s 55ms/step - loss: -0.1046 - val_loss: 0.1872\n"
     ]
    }
   ],
   "source": [
    "ytrain = np.zeros(np.shape(Xtrain)[0])\n",
    "yvalid = np.zeros(np.shape(Xvalid)[0])\n",
    "\n",
    "\n",
    "\n",
    "adamOpti = tensorflow.keras.optimizers.Adam (learning_rate = 0.001)\n",
    "merged3.compile(optimizer = adamOpti, loss = CustomLoss())\n",
    "\n",
    "history = merged3.fit(x = [Xtrain*1e-7, Xtrain_flip*1e-7], y = ytrain, \\\n",
    "                    validation_data=([Xvalid*1e-7, Xvalid_flip*1e-7], yvalid), epochs = 100, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9820aca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce2d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b38d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77699807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
